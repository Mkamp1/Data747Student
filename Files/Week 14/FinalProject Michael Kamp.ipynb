{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a367b63-0c64-43d9-b13b-d82fb101ea1c",
   "metadata": {},
   "source": [
    "# Predicting Youth Mental Health Risk\n",
    "**Student:** Michael Kamp\n",
    "\n",
    "# 1. Business Understanding\n",
    "\n",
    "This notebook supports the final project for **DATA 747** and focuses on predicting poor mental-health outcomes among U.S. high-school students using the **2019 CDC Youth Risk Behavior Surveillance System (YRBSS)** dataset.\n",
    "\n",
    "The primary goal of this analysis is to examine behavioral and demographic factors associated with the likelihood that a student reports **persistent sadness or hopelessness (QN8)**. Using this dataset, we prepare the data, conduct exploratory analyses, train supervised machine-learning models, and evaluate their predictive performance.\n",
    "\n",
    "### **Research Question**\n",
    "**What factors help predict whether a high-school student reports poor mental health, defined as persistent sadness or hopelessness for two or more consecutive weeks?**\n",
    "\n",
    "\n",
    "## **Notebook Roadmap**\n",
    "\n",
    "1. **Data Understanding**  \n",
    "   Review dataset structure, coding conventions, and key variables.\n",
    "\n",
    "2. **Data Preparation**  \n",
    "   Clean the dataset, handle missing values, recode variables, and prepare inputs for modeling.\n",
    "\n",
    "3. **Exploratory Data Analysis (EDA)**  \n",
    "   Visualize distributions and examine relationships among behavioral, demographic, and health variables.\n",
    "\n",
    "4. **Modeling**  \n",
    "   Train and evaluate two supervised classification models: logistic regression and decision tree.\n",
    "\n",
    "5. **Model Evaluation**  \n",
    "   Assess predictive performance using confusion matrices, ROC curves, classification metrics, and feature importance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af760a8-db82-49e4-b56d-00e9b947dd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# GLOBAL WARNING SUPPRESSION (Clean Notebook)\n",
    "# =============================================\n",
    "import warnings\n",
    "\n",
    "# Suppress scikit-learn feature name warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"X has feature names\")\n",
    "\n",
    "# Suppress all UserWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Suppress seaborn/matplotlib FutureWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# =============================================\n",
    "# STANDARD IMPORTS\n",
    "# =============================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Paths to the raw fixed-width data and SAS layout file\n",
    "data_file = \"yrbs2019.dat\"\n",
    "sas_layout = \"yrbs2019_input.sas\"\n",
    "\n",
    "\n",
    "def parse_sas_input_pointer_format(file_path):\n",
    "    \"\"\"\n",
    "    Parse a CDC SAS INPUT file in pointer format (e.g., @1 Q1 2.)\n",
    "    and return (colspecs, names) for use with pandas.read_fwf.\n",
    "    \"\"\"\n",
    "    colspecs = []\n",
    "    names = []\n",
    "    pattern = re.compile(r\"@(\\d+)\\s+(\\w+)\\s+(\\$?)(\\d+)\")\n",
    "\n",
    "    entries = []\n",
    "\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            match = pattern.search(line)\n",
    "            if match:\n",
    "                start = int(match.group(1)) - 1  # convert 1-based to 0-based index\n",
    "                name = match.group(2)\n",
    "                width = int(match.group(4))\n",
    "                end = start + width\n",
    "                entries.append((start, end, name))\n",
    "\n",
    "    # Sort by starting position\n",
    "    entries.sort(key=lambda x: x[0])\n",
    "\n",
    "    # Build colspecs and column names\n",
    "    for start, end, name in entries:\n",
    "        colspecs.append((start, end))\n",
    "        names.append(name)\n",
    "\n",
    "    return colspecs, names\n",
    "\n",
    "\n",
    "# Parse SAS layout\n",
    "colspecs, names = parse_sas_input_pointer_format(sas_layout)\n",
    "print(f\"Columns detected: {len(names)}\")\n",
    "\n",
    "# Load fixed-width YRBS 2019 dataset\n",
    "data = pd.read_fwf(data_file, colspecs=colspecs, names=names)\n",
    "\n",
    "print(\"First 10 rows:\")\n",
    "display(data.head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b9f948-6af3-4511-a7ae-5585f73343c1",
   "metadata": {},
   "source": [
    "## 2. Data Understanding\n",
    "\n",
    "This section provides an initial examination of the 2019 CDC Youth Risk Behavior Surveillance System (YRBSS) dataset. The dataset contains a wide range of demographic, behavioral, and health-related variables reported by high-school students across the United States. Before preparing the data for modeling, it is essential to understand the structure, content, and quality of the dataset.\n",
    "\n",
    "### Objectives of Data Understanding\n",
    "1. **Confirm that the dataset loaded correctly** using the CDC SAS layout file to extract column positions and variable names.\n",
    "2. **Review dataset structure**, including the number of rows, columns, and variable data types.\n",
    "3. **Identify major characteristics of key demographic variables** such as sex (Q2), grade (Q3), and race/ethnicity (Q4).\n",
    "4. **Examine the distribution of the mental-health target variable (QN8)**, which indicates whether a student felt sad or hopeless for two or more weeks.\n",
    "5. **Assess missing data**, which is important because the YRBSS uses special numeric codes (e.g., 7, 8, 9, 77, 88, 99) to represent non-responses.\n",
    "6. **Explore early distributions** of selected behavioral and health variables relevant to the research question.\n",
    "\n",
    "### Why This Matters\n",
    "A thorough understanding of the dataset ensures that the next step—data preparation—is performed accurately and effectively. Identifying missing values, variable types, and preliminary patterns informs decisions about cleaning, recoding, and selecting features for modeling.\n",
    "\n",
    "The visualizations and summary statistics in this section help lay the foundation for exploratory data analysis and model development in later sections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad04172e-300d-489f-9d19-90c28a4a6ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION 2 — DATA UNDERSTANDING\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Preview of dataset:\")\n",
    "display(data.head())\n",
    "\n",
    "print(\"\\nSummary statistics:\")\n",
    "display(data.describe(include=\"all\"))\n",
    "\n",
    "print(\"\\nData types:\")\n",
    "display(data.dtypes)\n",
    "\n",
    "print(\"\\nMissing values in each column:\")\n",
    "display(data.isnull().sum())\n",
    "\n",
    "# Key demographic variables for context\n",
    "categorical_cols = [\"Q2\", \"Q3\", \"Q6\", \"RACEETH\"]\n",
    "\n",
    "print(\"\\nUnique values for key categorical variables:\")\n",
    "for col in categorical_cols:\n",
    "    if col in data.columns:\n",
    "        print(f\"{col}: {data[col].unique()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46726a44-f726-4151-aeb6-ba186f5b5eaa",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "\n",
    "This section prepares the YRBSS dataset for modeling by cleaning the data, selecting relevant variables, handling missing values, encoding categorical fields, and splitting the data into training and testing sets. Because the dataset is stored in a fixed-width format and includes CDC-coded missing values, several preprocessing steps are required before fitting machine-learning models.\n",
    "\n",
    "### Objectives of Data Preparation\n",
    "1. **Clean and standardize the dataset** by replacing CDC-coded missing values with `NaN`.\n",
    "2. **Select the variables most relevant to predicting mental-health outcomes**, including demographic, behavioral, and health indicators.\n",
    "3. **Convert categorical variables** (e.g., sex, grade, age group, race/ethnicity) into appropriate data types.\n",
    "4. **Transform categorical fields using one-hot encoding** so they can be used by machine-learning algorithms.\n",
    "5. **Drop incomplete rows** to ensure a consistent and reliable modeling dataset.\n",
    "6. **Separate features (X) from the target variable (y)** and prepare them for model training.\n",
    "7. **Scale numeric predictors** using StandardScaler for the logistic regression model.\n",
    "8. **Create a train–test split** using stratified sampling to preserve the distribution of the mental-health outcome.\n",
    "\n",
    "### Why These Steps Are Necessary\n",
    "The YRBSS dataset uses a mixture of coded categorical values and non-standard missing-data indicators. Preparing the data properly ensures that:\n",
    "\n",
    "- the machine-learning models receive consistent and meaningful inputs,\n",
    "- categorical variables are encoded without introducing bias,\n",
    "- scaled variables improve optimization for logistic regression,\n",
    "- and the training/testing evaluation reflects the true distribution of mental-health outcomes.\n",
    "\n",
    "At the end of this section, the dataset is fully prepared for modeling using logistic regression and decision-tree classifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0a4259-53fb-407f-b11c-fa6469240678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION 3 — DATA PREPARATION\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Replace CDC-coded missing values with NaN\n",
    "coded_missing = [7, 8, 9, 77, 88, 99]\n",
    "data = data.replace(coded_missing, np.nan)\n",
    "\n",
    "# 2. Select relevant variables for modeling\n",
    "selected_columns = [\n",
    "    \"Q2\",        # Sex\n",
    "    \"Q3\",        # Grade\n",
    "    \"Q6\",        # Age category\n",
    "    \"RACEETH\",   # Race / Ethnicity\n",
    "\n",
    "    \"QN8\",       # Target: sad/hopeless ≥ 2 weeks\n",
    "\n",
    "    \"QN12\",      # Alcohol use\n",
    "    \"QNDAYEVP\",  # E-cigarette use (days)\n",
    "    \"QNDAYCIG\",  # Cigarette use (days)\n",
    "    \"QNSODA1\",   # Soda consumption\n",
    "    \"QNPA0DAY\",  # No physical activity\n",
    "    \"QNOBESE\",   # Obesity status\n",
    "    \"BMIPCT\"     # BMI percentile\n",
    "]\n",
    "\n",
    "# Keep only columns that exist in the parsed dataset\n",
    "selected_columns = [c for c in selected_columns if c in data.columns]\n",
    "cleaned = data[selected_columns].copy()\n",
    "\n",
    "# 3. Convert demographic variables to categorical\n",
    "categorical_vars = [\"Q2\", \"Q3\", \"Q6\", \"RACEETH\"]\n",
    "\n",
    "for col in categorical_vars:\n",
    "    if col in cleaned.columns:\n",
    "        cleaned[col] = cleaned[col].astype(\"category\")\n",
    "\n",
    "# 4. Convert remaining variables to numeric\n",
    "for col in cleaned.columns:\n",
    "    if col not in categorical_vars:\n",
    "        cleaned[col] = pd.to_numeric(cleaned[col], errors=\"coerce\")\n",
    "\n",
    "# 5. Drop rows with missing values for modeling\n",
    "rows_before = cleaned.shape[0]\n",
    "cleaned = cleaned.dropna()\n",
    "rows_after = cleaned.shape[0]\n",
    "\n",
    "print(f\"Rows before dropping missing values: {rows_before}\")\n",
    "print(f\"Rows after dropping missing values:  {rows_after}\")\n",
    "print(f\"Total rows removed:                  {rows_before - rows_after}\")\n",
    "\n",
    "# 6. Split into features (X) and target (y)\n",
    "#    QN8 == 1 indicates persistent sadness / hopelessness\n",
    "y = (cleaned[\"QN8\"] == 1).astype(int)\n",
    "X = cleaned.drop(\"QN8\", axis=1)\n",
    "\n",
    "# 7. One-hot encode categorical variables\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "print(\"Shape after encoding:\", X.shape)\n",
    "\n",
    "# 8. Train/Test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.25,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape)\n",
    "print(\"Training target distribution:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "# 9. Standard Scaling (for Logistic Regression)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nScaling complete.\")\n",
    "print(\"Scaled X_train shape:\", X_train_scaled.shape)\n",
    "print(\"Scaled X_test shape:\", X_test_scaled.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab7d907-a38a-43b5-9136-ee8b626f7d89",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)\n",
    "\n",
    "This section explores the structure and key characteristics of the variables included in the study. Exploratory Data Analysis helps identify patterns,\n",
    "trends, and potential relationships in the dataset prior to modeling. Because the YRBSS includes a broad range of demographic, behavioral,and health variables,\n",
    "EDA is essential for understanding which factors may be associated with poor mental-health outcomes.\n",
    "\n",
    "### Objectives of EDA\n",
    "1. **Examine the distribution of demographic variables**, including sex (Q2), grade level (Q3), and race/ethnicity (Q4), to understand population composition.\n",
    "2. **Visualize the distribution of the target variable (QN8)**, which indicates whether a student experienced persistent sadness or hopelessness.\n",
    "3. **Explore behavioral indicators** such as alcohol use, vaping, smoking, soda consumption, and physical inactivity.\n",
    "4. **Compare mental-health outcomes across demographic and behavioral subgroups** using side-by-side count plots.\n",
    "5. **Generate a correlation heatmap** to identify relationships among numeric variables, such as BMI percentile and behavioral frequency indicators.\n",
    "\n",
    "### Why This Matters\n",
    "EDA provides an essential foundation for modeling. Visualizing these variables allows us to quickly identify imbalances, unusual values, and potential predictors. \n",
    "Understanding how mental-health outcomes vary across demographic and behavioral groups helps guide feature selection and interpret model performance in later sections.\n",
    "\n",
    "The visualizations in this section highlight important structures within the dataset and support the development of more effective predictive models.\n",
    "Because the target variable (QN8) is highly imbalanced, EDA also helps confirm the minority class distribution and informs how model performance should be evaluated in later sections.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e09f53a-573b-4eea-a59a-6a07124420fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION 4 — EXPLORATORY DATA ANALYSIS (EDA)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set a clean style for all charts\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4.1 Distribution of the Target Variable (QN8)\n",
    "# ---------------------------------------------\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x=y, palette=\"pastel\")\n",
    "plt.title(\"Distribution of QN8 (Sad/Hopeless ≥ 2 Weeks)\")\n",
    "plt.xlabel(\"QN8: 0 = No, 1 = Yes\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4.2 Distributions of Numerical Features\n",
    "# ---------------------------------------------\n",
    "numeric_vars = [\"BMIPCT\", \"QNDAYEVP\", \"QNDAYCIG\", \"QNSODA1\"]\n",
    "\n",
    "for col in numeric_vars:\n",
    "    if col in cleaned.columns:\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.histplot(cleaned[col], kde=True, color=\"skyblue\")\n",
    "        plt.title(f\"Distribution of {col}\")\n",
    "        plt.xlabel(col)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4.3 Count Plots for Categorical Variables\n",
    "# ---------------------------------------------\n",
    "categorical_vars = [\"Q2\", \"Q3\", \"Q6\", \"RACEETH\"]\n",
    "\n",
    "for col in categorical_vars:\n",
    "    if col in cleaned.columns:\n",
    "        plt.figure(figsize=(7, 4))\n",
    "        sns.countplot(data=cleaned, x=col, palette=\"muted\")\n",
    "        plt.title(f\"Distribution of {col}\")\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4.4 QN8 vs Demographic Groups\n",
    "# ---------------------------------------------\n",
    "for col in categorical_vars:\n",
    "    if col in cleaned.columns:\n",
    "        plt.figure(figsize=(7, 4))\n",
    "        sns.countplot(data=cleaned, x=col, hue=\"QN8\", palette=\"Set2\")\n",
    "        plt.title(f\"QN8 by {col}\")\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.legend(title=\"QN8\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4.5 Correlation Heatmap (Numerical Variables)\n",
    "# ---------------------------------------------\n",
    "numeric_for_corr = [\"BMIPCT\", \"QNDAYEVP\", \"QNDAYCIG\", \"QNSODA1\", \"QNPA0DAY\", \"QNOBESE\"]\n",
    "\n",
    "numeric_for_corr = [col for col in numeric_for_corr if col in cleaned.columns]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cleaned[numeric_for_corr].corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation Heatmap (Numerical Predictors)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bafc57-7315-4387-af77-19ed1600b1ea",
   "metadata": {},
   "source": [
    "## 5. Modeling\n",
    "\n",
    "In this section, we build predictive models to identify factors associated with poor mental-health outcomes among high-school students. Using the cleaned and prepared dataset from the previous section, we apply two supervised machine-learning algorithms:\n",
    "\n",
    "1. **Logistic Regression** – A linear classification model used as a baseline to predict the likelihood that a student reports persistent sadness or hopelessness (QN8). Logistic regression is effective when the relationship between predictors and the target variable is approximately linear and when interpretability is important.\n",
    "\n",
    "2. **Decision Tree Classifier** – A non-linear model capable of capturing complex interactions between demographic, behavioral, and health variables. Decision trees can identify the most influential predictors by recursively splitting the dataset into meaningful subgroups.\n",
    "\n",
    "### Modeling Workflow\n",
    "- The dataset is split into training and testing sets using stratified sampling to preserve the distribution of the mental-health outcome.\n",
    "- Logistic regression is trained on **scaled** inputs to improve stability and performance.\n",
    "- The decision-tree classifier is trained on **unscaled** inputs, since tree-based models are not sensitive to feature scaling.\n",
    "- Each model generates predictions on the testing set and is evaluated using accuracy, precision, recall, F1-score, and a classification report.\n",
    "\n",
    "### Purpose of This Section\n",
    "By fitting both a linear and a non-linear model, we can compare performance and better understand the predictive structure of the YRBSS dataset. This provides insight into which model is best suited for identifying students at elevated mental-health risk.  Because the target variable (QN8) is highly imbalanced, evaluating models using precision, recall, F1-score, and AUC offers a clearer picture of how effectively each approach identifies the minority class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118a1422-23e9-42bf-b768-ee8b79fbbde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION 5 — MODELING\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 5.1 Logistic Regression (requires scaled features)\n",
    "# ----------------------------------------------------\n",
    "log_model = LogisticRegression(\n",
    "    max_iter=500,\n",
    "    solver='lbfgs',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "log_model.fit(X_train_scaled, y_train)\n",
    "log_pred = log_model.predict(X_test_scaled)\n",
    "log_pred_proba = log_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(\"Logistic Regression model training complete.\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 5.2 Decision Tree Classifier (raw, unscaled features)\n",
    "# ----------------------------------------------------\n",
    "tree_model = DecisionTreeClassifier(\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "tree_model.fit(X_train, y_train)\n",
    "tree_pred = tree_model.predict(X_test)\n",
    "tree_pred_proba = tree_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Decision Tree model training complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda5ca5c-8f91-4895-b26b-48511dbe8139",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation\n",
    "\n",
    "After training the logistic regression and decision-tree models, this section evaluates their performance on the testing dataset. Model evaluation provides insight into how effectively each classifier identifies students who report poor mental-health outcomes and how well the models generalize to unseen data.\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "To assess each model, we use several standard classification performance measures:\n",
    "\n",
    "• Confusion Matrix\n",
    "\n",
    "Shows counts of true positives, true negatives, false positives, and false negatives\n",
    "This matrix helps reveal how frequently each model correctly identifies students with and without reported sadness or hopelessness.\n",
    "\n",
    "• Classification Report\n",
    "\n",
    "Includes precision, recall, F1-score, and class-specific support.\n",
    "These metrics are especially important given the strong class imbalance in the mental-health outcome (QN8).\n",
    "\n",
    "• ROC Curve (Receiver Operating Characteristic)\n",
    "\n",
    "Illustrates the trade-off between the true positive rate and false positive rate across classification thresholds.\n",
    "\n",
    "• AUC (Area Under the Curve)\n",
    "\n",
    "Measures how well the model separates students who reported persistent sadness from those who did not.\n",
    "Higher AUC values indicate stronger discriminative ability.\n",
    "\n",
    "• Feature Importance (Decision Tree)\n",
    "\n",
    "Identifies which demographic, behavioral, or health-related variables contribute most strongly to the decision-tree model’s predictions.\n",
    "\n",
    "### Purpose of This Section\n",
    "This evaluation determines which model performs best and provides insight into the underlying predictive patterns within the dataset. Understanding model strengths and limitations helps guide interpretation of the results in the accompanying APA report. Given the imbalanced nature of the target variable (QN8), metrics such as recall, precision, F1-score, and AUC are essential for assessing how effectively each model identifies the minority class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45bfd1a-9c14-4c43-b508-6999a79e0a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# SECTION 6 – MODEL EVALUATION\n",
    "# ============================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    roc_curve,\n",
    "    roc_auc_score\n",
    ")\n",
    "import seaborn as sns\n",
    "\n",
    "# ---------------------------------------\n",
    "# 1. Logistic Regression – Predictions\n",
    "# ---------------------------------------\n",
    "log_probs = log_model.predict_proba(X_test)[:, 1]      # Probabilities for ROC/AUC\n",
    "log_pred = log_model.predict(X_test)                   # Class predictions\n",
    "log_fpr, log_tpr, _ = roc_curve(y_test, log_probs)\n",
    "log_auc = roc_auc_score(y_test, log_probs)\n",
    "\n",
    "# ---------------------------------------\n",
    "# 2. Decision Tree – Predictions\n",
    "# ---------------------------------------\n",
    "tree_probs = tree_model.predict_proba(X_test)[:, 1]\n",
    "tree_pred = tree_model.predict(X_test)\n",
    "tree_fpr, tree_tpr, _ = roc_curve(y_test, tree_probs)\n",
    "tree_auc = roc_auc_score(y_test, tree_probs)\n",
    "\n",
    "# ---------------------------------------\n",
    "# PRINT EVALUATION METRICS\n",
    "# ---------------------------------------\n",
    "print(\"===== LOGISTIC REGRESSION METRICS =====\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, log_pred))\n",
    "print(\"AUC:\", log_auc)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, log_pred))\n",
    "\n",
    "print(\"\\n===== DECISION TREE METRICS =====\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, tree_pred))\n",
    "print(\"AUC:\", tree_auc)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, tree_pred))\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# PLOT 1 – CONFUSION MATRIX (LOGISTIC REGRESSION)\n",
    "# ============================================\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, log_pred, cmap=\"Blues\", ax=ax)\n",
    "ax.set_title(\"Confusion Matrix — Logistic Regression\")\n",
    "\n",
    "fig.savefig(\"confusion_logistic.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# PLOT 2 – CONFUSION MATRIX (DECISION TREE)\n",
    "# ============================================\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, tree_pred, cmap=\"Greens\", ax=ax)\n",
    "ax.set_title(\"Confusion Matrix — Decision Tree\")\n",
    "\n",
    "fig.savefig(\"confusion_tree.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# PLOT 3 – ROC CURVE FOR BOTH MODELS\n",
    "# ============================================\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "ax.plot(log_fpr, log_tpr, label=f\"Logistic Regression (AUC = {log_auc:.3f})\")\n",
    "ax.plot(tree_fpr, tree_tpr, label=f\"Decision Tree (AUC = {tree_auc:.3f})\")\n",
    "ax.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "\n",
    "ax.set_xlabel(\"False Positive Rate\")\n",
    "ax.set_ylabel(\"True Positive Rate\")\n",
    "ax.set_title(\"ROC Curve for Models\")\n",
    "ax.legend()\n",
    "\n",
    "fig.savefig(\"roc_curve.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# CREATE FEATURE IMPORTANCE DATAFRAME (Decision Tree)\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Importance\": tree_model.feature_importances_\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# PLOT 4 – FEATURE IMPORTANCE (DECISION TREE)\n",
    "# ============================================\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "sns.barplot(\n",
    "    data=importance_df.head(15),\n",
    "    x=\"Importance\",\n",
    "    y=\"Feature\",\n",
    "    palette=\"viridis\",\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_title(\"Top 15 Most Important Features — Decision Tree\")\n",
    "\n",
    "fig.savefig(\"feature_importance.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853c773a-59c1-4739-918c-39cc725211c8",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "This analysis compared logistic regression and decision-tree classification models to evaluate their ability to predict persistent sadness among high-school students using the 2019 YRBSS dataset. While the decision tree achieved higher overall accuracy, both models struggled to identify the minority class, reflecting the underlying class imbalance in the dataset. The evaluation metrics highlighted important strengths and limitations of each model and provided insight into which behavioral and demographic factors may contribute to elevated mental-health risk among adolescents. These findings support the need for additional techniques—such as resampling, ensemble methods, or alternative algorithms—to improve minority-class detection in future work.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
