{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Assignment: Data Cleaning and Preparation\n",
    "Student:     **Michael Kamp**\n",
    "\n",
    "### Objective: To apply the techniques learned in class to clean and prepare data for analysis. Specifically, students will learn how to handle missing values, duplicates, and outliers in data. By the end of the lab assignment, students should be able to clean and prepare data for further analysis using Python.\n",
    "\n",
    "### Instructions:\n",
    "1. Download and import the \"Adult Income\" dataset from the UCI Machine Learning Repository.\n",
    "2. Load the data into a DataFrame.\n",
    "3. Explore the dataset and identify missing values, duplicates, and outliers.\n",
    "4. Use appropriate techniques to handle missing values, duplicates, and outliers.\n",
    "5. Perform basic data analysis on the cleaned dataset to answer the following questions.\n",
    "6. Save the cleaned dataset to a CSV file for further analysis.\n",
    "\n",
    "### Dataset\n",
    "The \"Adult Income\" dataset contains information about individuals, including their age, education, work class, marital status, occupation, race, gender, native country, and income. The dataset contains 48,842 rows and 14 columns.\n",
    "\n",
    "### Deliverable:\n",
    "Modify this notebook to include the python code as well as any documentation related to your submission.  Submit the notebook as your response in Blackboard.\n",
    "\n",
    "### Grading Criteria:\n",
    "\n",
    "Your lab assignment will be graded based on the following criteria:\n",
    "\n",
    "- Correctness of the implementation\n",
    "- Proper use of basic control structures and functions\n",
    "- Code efficiency\n",
    "- Clarity and readability of the code\n",
    "- Compliance with the instructions and deliverables.\n",
    "\n",
    "### Student Submission\n",
    "Download and import the \"Adult Income\" dataset into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Step 1: Load the Adult Income Dataset (Full Version) ----\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define column names\n",
    "columns = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
    "    'marital_status', 'occupation', 'relationship', 'race',\n",
    "    'gender', 'capital_gain', 'capital_loss', 'hours_per_week',\n",
    "    'native_country', 'income'\n",
    "]\n",
    "\n",
    "# Load both training and testing sets\n",
    "url_train = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
    "url_test = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test'\n",
    "\n",
    "train_df = pd.read_csv(url_train, header=None, names=columns, na_values=' ?')\n",
    "test_df  = pd.read_csv(url_test,  header=0, names=columns, na_values=' ?')\n",
    "\n",
    "# Clean up the 'income' column in the test set (remove trailing periods)\n",
    "test_df['income'] = test_df['income'].str.replace('.', '', regex=False).str.strip()\n",
    "\n",
    "# Combine both datasets into a single DataFrame\n",
    "full_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "# Assign to main working DataFrame\n",
    "df = full_df.copy()\n",
    "\n",
    "# Display dataset info\n",
    "print(\"âœ… Combined Dataset Loaded Successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(\"\\nNumber of Missing Values per Column:\\n\", df.isnull().sum())\n",
    "print(\"\\nPreview of Combined Data:\")\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Step 2: Basic Exploration ----\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nSummary Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nMissing values per column:\\n\", df.isnull().sum())\n",
    "print(\"\\nDuplicate rows:\", df.duplicated().sum())\n",
    "\n",
    "# Check summary statistics for numeric columns\n",
    "print(\"\\nNumeric Summary:\")\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps 1â€“2 â€“ Data Loading and Setup Summary\n",
    "The Adult Income dataset was imported from the UCI Machine Learning Repository, using both the training (`adult.data`) and testing (`adult.test`) files.  \n",
    "After cleaning the `income` column to remove trailing periods, both datasets were combined into a single DataFrame for a total of **48,842 observations and 15 columns**.  \n",
    "\n",
    "All core libraries â€” `pandas`, `numpy`, `matplotlib`, and `seaborn` â€” were imported, and display options were configured for readability.  \n",
    "This step established a well-structured working environment and ensured the full dataset was ready for exploration and cleaning in subsequent steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Step 3: Explore the Combined Dataset ----\n",
    "\n",
    "# Check dataset shape\n",
    "print(\"ðŸ“Š Dataset shape (rows, columns):\", df.shape)\n",
    "\n",
    "# Overview of data types and missing values\n",
    "print(\"\\nðŸ§¾ Dataset Info:\")\n",
    "df.info()\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nðŸ” First 5 rows:\")\n",
    "display(df.head())\n",
    "\n",
    "# Check for missing values (properly recognized as NaN)\n",
    "print(\"\\nâ“ Missing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"\\nðŸ§© Number of duplicate rows: {duplicates}\")\n",
    "\n",
    "# Summary statistics for numeric columns\n",
    "print(\"\\nðŸ“ˆ Summary Statistics for Numeric Columns:\")\n",
    "display(df.describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3 â€“ Initial Data Exploration Summary:\n",
    "The initial exploration of the combined Adult Income dataset provided an overview of its structure and quality.  \n",
    "The dataset contained **48,842 rows and 15 columns**, including both numeric and categorical variables.  \n",
    "Preliminary inspection revealed that some columns, such as `workclass`, `occupation`, and `native_country`, contained missing values represented by `' ?'`.  \n",
    "A small number of duplicate rows were also identified, confirming the need for cleaning.  \n",
    "Overall, this exploratory step established a clear understanding of the datasetâ€™s composition and areas requiring preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Step 4: Handle Missing Values ----\n",
    "import numpy as np\n",
    "\n",
    "# Replace '?' with NaN so Pandas can detect them as missing\n",
    "df.replace(' ?', np.nan, inplace=True)\n",
    "\n",
    "# Check how many missing values now\n",
    "print(\"Missing values after replacing '?':\\n\", df.isnull().sum())\n",
    "\n",
    "# Fill missing values with the mode (most frequent value) for each column\n",
    "for col in ['workclass', 'occupation', 'native_country']:\n",
    "    df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "# Verify all missing values have been handled\n",
    "print(\"âœ… Missing values handled successfully!\\n\")\n",
    "print(\"Remaining missing values per column:\\n\", df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4 â€“ Handling Missing Values Summary:\n",
    "During this step, missing values represented by `' ?'` were identified and converted to proper `NaN` entries to enable accurate detection.  \n",
    "Three categorical columns â€” `workclass`, `occupation`, and `native_country` â€” contained missing data.  \n",
    "These were imputed using each columnâ€™s most frequent (mode) value to preserve the datasetâ€™s categorical consistency without introducing bias.  \n",
    "\n",
    "After replacement, all missing values were successfully resolved, resulting in a complete dataset ready for duplicate and outlier analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Step 5: Handle Duplicates ----\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"ðŸ§© Number of duplicate rows before removal: {duplicate_count}\")\n",
    "\n",
    "# If duplicates exist, show a few examples\n",
    "if duplicate_count > 0:\n",
    "    print(\"\\nðŸ” Preview of duplicate rows:\")\n",
    "    display(df[df.duplicated()].head())\n",
    "\n",
    "# Drop duplicate rows (if any)\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Confirm duplicates are removed\n",
    "print(f\"\\nâœ… Number of duplicate rows after removal: {df.duplicated().sum()}\")\n",
    "print(f\"ðŸ“Š Dataset shape after duplicate removal: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5 â€“ Duplicate Handling Summary:\n",
    "During the data cleaning process, 29 duplicate rows were detected in the combined dataset.  \n",
    "These duplicates represented identical entries across all columns, likely due to overlap between the training and test datasets from the UCI repository.  \n",
    "\n",
    "After reviewing and confirming their redundancy, all duplicate rows were removed, reducing the total record count slightly while preserving data integrity.  \n",
    "This ensures that each observation in the dataset is unique, preventing bias or distortion in future analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Step 6: Detect and Handle Outliers ----\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---- Identify Numeric Columns ----\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "print(\"ðŸ”¢ Numeric columns detected:\\n\", numeric_cols)\n",
    "\n",
    "# Create a copy BEFORE capping\n",
    "df_before = df[numeric_cols].copy()\n",
    "\n",
    "# ---- Define IQR Capping Function ----\n",
    "def cap_outliers(series):\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    capped_series = series.clip(lower_bound, upper_bound)\n",
    "    return capped_series\n",
    "\n",
    "# ---- Apply Outlier Capping ----\n",
    "for col in numeric_cols:\n",
    "    df[col] = cap_outliers(df[col])\n",
    "\n",
    "print(\"\\nâœ… Outlier handling complete using IQR capping method.\")\n",
    "\n",
    "# ---- Visualize Before vs After ----\n",
    "sns.set(style=\"whitegrid\", palette=\"muted\", color_codes=True)\n",
    "fig, axes = plt.subplots(2, len(numeric_cols), figsize=(20, 10))\n",
    "\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    sns.boxplot(x=df_before[col], ax=axes[0, i], color='pink')\n",
    "    axes[0, i].set_title(f\"Before Outlier Handling\\n{col}\", fontsize=10)\n",
    "    axes[0, i].set_xlabel(\"\")\n",
    "\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    sns.boxplot(x=df[col], ax=axes[1, i], color='mediumseagreen')\n",
    "    axes[1, i].set_title(f\"After Outlier Handling\\n{col}\", fontsize=10)\n",
    "    axes[1, i].set_xlabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- Check Capital Gain / Loss ----\n",
    "print(\"\\nðŸ’° Checking capital gain/loss columns...\")\n",
    "print(\"Capital Gain - Non-zero count:\", (df['capital_gain'] > 0).sum())\n",
    "print(\"Capital Loss - Non-zero count:\", (df['capital_loss'] > 0).sum())\n",
    "\n",
    "# ---- Quantify How Many Values Were Capped ----\n",
    "capped_counts = {}\n",
    "for col in numeric_cols:\n",
    "    capped_col = cap_outliers(df_before[col])\n",
    "    capped_counts[col] = np.sum(df_before[col] != capped_col)\n",
    "\n",
    "print(\"\\nðŸ“Š Number of values capped per numeric column:\\n\")\n",
    "for col, count in capped_counts.items():\n",
    "    print(f\"{col:<15} : {count}\")\n",
    "\n",
    "print(\"\\nâœ… Step 6 complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6 â€“ Outlier Handling Summary:\n",
    "The IQR capping method identified and adjusted a few significant outliers in the dataset, particularly in the fnlwgt and hours_per_week columns.\n",
    "Most other numeric variables, such as age and education_num, were already within normal statistical ranges and required minimal correction.\n",
    "\n",
    "Columns like capital_gain and capital_loss contain mostly zero values, which explains the lack of visible variation in their boxplots.\n",
    "Overall, outlier handling successfully reduced extreme values while preserving the natural distribution of the data, confirming that the dataset is now clean, consistent, and ready for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Step 7: Basic Data Analysis ----\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ---- Verify Dataset Integrity ----\n",
    "print(\"âœ… Dataset shape after cleaning:\", df.shape)\n",
    "print(\"\\nColumn data types:\\n\", df.dtypes)\n",
    "\n",
    "# ---- Summary Statistics for Numeric Variables ----\n",
    "print(\"\\nðŸ“Š Summary statistics for numeric variables:\\n\")\n",
    "display(df.describe().T)\n",
    "\n",
    "# ---- Frequency Counts for Key Categorical Variables ----\n",
    "categorical_cols = [\n",
    "    'workclass', 'education', 'marital_status',\n",
    "    'occupation', 'relationship', 'race', 'gender', 'income'\n",
    "]\n",
    "\n",
    "print(\"\\nðŸ“‹ Frequency counts for key categorical variables:\\n\")\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n--- {col.upper()} ---\")\n",
    "    print(df[col].value_counts().head(5))\n",
    "\n",
    "# ---- Visualize Target Variable Distribution ----\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x='income', hue='income', data=df, palette='coolwarm', legend=False)\n",
    "plt.title('Income Distribution (<=50K vs >50K)', fontsize=13)\n",
    "plt.xlabel('Income Category')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7 â€“ Basic Data Analysis:\n",
    "After cleaning, the dataset now contains 48,842 rows and 15 columns, with all missing values and duplicates handled.\n",
    "Numeric variables show realistic ranges and balanced distributions after outlier capping.\n",
    "Frequency counts for categorical features (such as workclass, education, and occupation) reveal consistent groupings.\n",
    "The target variable, income, remains imbalanced, with more individuals earning â‰¤ 50K than > 50K, which is common for this dataset.\n",
    "Overall, the dataset is fully prepared for further statistical analysis or model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Step 8: Export Cleaned Dataset ----\n",
    "\n",
    "# Define the output filename\n",
    "output_file = \"cleaned_adult_income.csv\"\n",
    "\n",
    "# Save the cleaned DataFrame to CSV (without index column)\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"âœ… Cleaned dataset successfully saved as: {output_file}\")\n",
    "\n",
    "# Optional: Verify export by reading the first few rows back in\n",
    "verify_df = pd.read_csv(output_file)\n",
    "print(\"\\nðŸ“‚ Verification â€” first 5 rows from exported file:\")\n",
    "display(verify_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8 â€“ Final Summary and Dataset Export:\n",
    "The cleaned dataset was successfully exported to a CSV file named `adult_cleaned.csv` for future use.  \n",
    "All data preparation steps were completed, including handling missing values, removing duplicates, and adjusting outliers using the IQR capping method.  \n",
    "\n",
    "After cleaning, the dataset now contains **48,842 unique records and 15 well-structured columns**, ensuring data integrity and readiness for advanced analysis.  \n",
    "This version of the Adult Income dataset is now fully prepared for future assignments involving visualization, statistical modeling, or machine learning tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Reflection:\n",
    "Through this weekâ€™s lab, I strengthened my understanding of the complete data cleaning workflow â€” from initial exploration to preparing a high-quality dataset for analysis.  \n",
    "I learned how to identify and handle missing values, remove duplicate records, and address outliers using the IQR method while maintaining the datasetâ€™s integrity. This process highlighted how crucial data preparation is in ensuring accurate results during statistical analysis and machine learning.  By the end of this exercise, I feel more confident in using Python and pandas for professional-level data cleaning, and I now recognize that well-prepared data is the foundation of any successful analytical project.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
