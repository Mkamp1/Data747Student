{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26380bae",
   "metadata": {},
   "source": [
    "# Week 11 – NLP with NLTK\n",
    "Michael Kamp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616ea2e3",
   "metadata": {},
   "source": [
    "## 1. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "266ee529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['After', 'a', 'long', 'week', 'of', 'studying', ',', 'Michael', 'took', 'a', 'break', 'with', 'his', 'VR', 'boxing', 'game', 'and', 'felt', 'energized', 'again', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"After a long week of studying, Michael took a break with his VR boxing game and felt energized again.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc102ec",
   "metadata": {},
   "source": [
    "## 2. Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8bd2b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['long', 'week', 'studying', ',', 'Michael', 'took', 'break', 'VR', 'boxing', 'game', 'felt', 'energized', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered = [w for w in tokens if w.lower() not in stop_words]\n",
    "print(filtered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f18a2af",
   "metadata": {},
   "source": [
    "## 3. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a36e18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['after', 'a', 'long', 'week', 'of', 'studi', ',', 'michael', 'took', 'a', 'break', 'with', 'hi', 'vr', 'box', 'game', 'and', 'felt', 'energ', 'again', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stems = [stemmer.stem(w) for w in tokens]\n",
    "print(stems)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f416223",
   "metadata": {},
   "source": [
    "## 4. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "221d0782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['After', 'a', 'long', 'week', 'of', 'studying', ',', 'Michael', 'took', 'a', 'break', 'with', 'his', 'VR', 'boxing', 'game', 'and', 'felt', 'energized', 'again', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemm = WordNetLemmatizer()\n",
    "lemmas = [lemm.lemmatize(w) for w in tokens]\n",
    "lemmas\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04e6a81",
   "metadata": {},
   "source": [
    "## 4b. POS-Aware Lemmatization (Improved Accuracy)\n",
    "\n",
    "In this enhanced version of lemmatization, each word is processed using its part-of-speech tag (POS).\n",
    "This significantly improves accuracy — especially for verbs like “took,” “felt,” and “studying,” which would not be lemmatized correctly without POS information.\n",
    "\n",
    "Using POS-aware lemmatization results in more meaningful base forms of words, which makes downstream NLP tasks such as text classification, topic modeling, and sentiment analysis more effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89a66957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('After', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('long', 'JJ'),\n",
       " ('week', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('studying', 'VBG'),\n",
       " (',', ','),\n",
       " ('Michael', 'NNP'),\n",
       " ('took', 'VBD'),\n",
       " ('a', 'DT'),\n",
       " ('break', 'NN'),\n",
       " ('with', 'IN'),\n",
       " ('his', 'PRP$'),\n",
       " ('VR', 'NNP'),\n",
       " ('boxing', 'NN'),\n",
       " ('game', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('felt', 'VBD'),\n",
       " ('energized', 'VBN'),\n",
       " ('again', 'RB'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Helper to convert NLTK POS tags → WordNet POS tags\n",
    "def get_wordnet_pos(tag, word=\"\"):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V') or word.lower() in ['felt', 'went', 'made']:\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # fallback\n",
    "\n",
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "# Use the same sentence as earlier\n",
    "text = \"After a long week of studying, Michael took a break with his VR boxing game and felt energized again.\"\n",
    "\n",
    "# Tokenize + POS tag\n",
    "tokens = nltk.word_tokenize(text)\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "# POS-aware lemmatization\n",
    "lemmas_pos_aware = [\n",
    "    lemm.lemmatize(word, get_wordnet_pos(tag, word)) \n",
    "    for word, tag in pos_tags\n",
    "]\n",
    "\n",
    "lemmas_pos_aware\n",
    "pos_tags\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb63af6",
   "metadata": {},
   "source": [
    "Interpretation\n",
    "\n",
    "POS-aware lemmatization correctly identifies words like:\n",
    "\n",
    "studying → study (verb)\n",
    "\n",
    "took → take (past tense verb)\n",
    "\n",
    "felt → feel (past tense verb)\n",
    "\n",
    "energized → energize (past participle)\n",
    "\n",
    "Compared to regular lemmatization, which leaves most words unchanged,\n",
    "POS-aware lemmatization produces cleaner, more informative text, which is crucial for accurate NLP analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c2724b",
   "metadata": {},
   "source": [
    "## 5. POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b3596ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('After', 'IN'), ('a', 'DT'), ('long', 'JJ'), ('week', 'NN'), ('of', 'IN'), ('studying', 'VBG'), (',', ','), ('Michael', 'NNP'), ('took', 'VBD'), ('a', 'DT'), ('break', 'NN'), ('with', 'IN'), ('his', 'PRP$'), ('VR', 'NNP'), ('boxing', 'NN'), ('game', 'NN'), ('and', 'CC'), ('felt', 'VBD'), ('energized', 'VBN'), ('again', 'RB'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "pos_tags = nltk.pos_tag(tokens)\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a0e243",
   "metadata": {},
   "source": [
    "## 6. Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd27e561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  After/IN\n",
      "  (NP a/DT long/JJ week/NN)\n",
      "  of/IN\n",
      "  studying/VBG\n",
      "  ,/,\n",
      "  Michael/NNP\n",
      "  took/VBD\n",
      "  (NP a/DT break/NN)\n",
      "  with/IN\n",
      "  his/PRP$\n",
      "  VR/NNP\n",
      "  (NP boxing/NN)\n",
      "  (NP game/NN)\n",
      "  and/CC\n",
      "  felt/VBD\n",
      "  energized/VBN\n",
      "  again/RB\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "parser = nltk.RegexpParser(grammar)\n",
    "chunks = parser.parse(pos_tags)\n",
    "print(chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e01b3c",
   "metadata": {},
   "source": [
    "## 7. Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19ff55bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  After/IN\n",
      "  a/DT\n",
      "  long/JJ\n",
      "  week/NN\n",
      "  of/IN\n",
      "  studying/VBG\n",
      "  ,/,\n",
      "  (PERSON Michael/NNP)\n",
      "  took/VBD\n",
      "  a/DT\n",
      "  break/NN\n",
      "  with/IN\n",
      "  his/PRP$\n",
      "  VR/NNP\n",
      "  boxing/NN\n",
      "  game/NN\n",
      "  and/CC\n",
      "  felt/VBD\n",
      "  energized/VBN\n",
      "  again/RB\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "ner = nltk.ne_chunk(pos_tags)\n",
    "print(ner)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481265bf",
   "metadata": {},
   "source": [
    "## 8. Summary & Key Takeaways\n",
    "\n",
    "This week’s NLP lab demonstrated the essential preprocessing steps used in text analytics and Natural Language Processing. These steps transform raw, unstructured text into structured forms that machine learning models can interpret.\n",
    "\n",
    "**Key concepts reviewed:**\n",
    "\n",
    "• **Tokenization** – Breaks a sentence into individual words or tokens.  \n",
    "• **Stopword Removal** – Removes common but uninformative words.  \n",
    "• **Stemming** – Reduces words to their root form.  \n",
    "• **Lemmatization** – Reduces words to their meaningful base form.  \n",
    "• **Part-of-Speech Tagging** – Labels each word with its grammatical role.  \n",
    "• **Chunking** – Groups tokens into meaningful phrases.  \n",
    "• **Named Entity Recognition (NER)** – Identifies people, places, dates, etc.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Anaconda Base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
