{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Assignment: Text Analytics / Natural Language Processing (NLP) with the NLTK library\n",
    "\n",
    "### Objective: To apply knowledge of text analytics and NLP in Python.\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "This will be part of a livecoding session.\n",
    "\n",
    "*Introductory Terms*\n",
    "- Corpus = Collection of documents / texts\n",
    "- Tokens = Individual elements (words)\n",
    "- Tokenizing = Process of breaking down tokens (sentence or word)\n",
    "- Stop Words = Dictionary of words to discard (a, an, the, if, etc.)\n",
    "- Stemming = Reducing words to their root (stem)\n",
    "- Parts of Speech = Noun, Pronoun, Verb, Adverb, etc.\n",
    "  - JJ = Adjectives \n",
    "  - NN = Nouns\n",
    "  - RB = Adverbs\n",
    "  - PRP = Pronouns\n",
    "  - VB = Verbs\n",
    "- Lemmatizing = Reducing words to their core meaning\n",
    "- Chunking = Allows you to identify phrases for context\n",
    "- Chinking = Exclude a pattern / opposite of chunking\n",
    "- Named Entity Recognition (NER) - Allows you to find named entites in your text\n",
    "\n",
    "*Great reference:  https://realpython.com/nltk-nlp-python/#:~:text=Natural%20language%20processing%20(NLP)%20is,and%20contains%20human%2Dreadable%20text.*\n",
    "\n",
    "### Deliverables:\n",
    "\n",
    "This Jupyter notebook.\n",
    "\n",
    "### Grading Criteria:\n",
    "\n",
    "1. Correctness and functionality of each function/program\n",
    "2. Proper use of basic control structures and functions\n",
    "3. Code readability and organization\n",
    "4. Note: Students are encouraged to work collaboratively, but each student must submit their own work. In addition, students should utilize version control (e.g. GitHub) to  manage their code and collaborate with peers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example from NLTK website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\mike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\mike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import Tree\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "sentence = \"\"\"At eight o'clock on Thursday morning... Arthur didn't feel very good.\"\"\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tokens\n",
    "# Output: ['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning', '...', 'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.']\n",
    "\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "tagged[0:6]\n",
    "# Output: [('At', 'IN'), ('eight', 'CD'), (\"o'clock\", 'JJ'), ('on', 'IN'), ('Thursday', 'NNP'), ('morning', 'NN')]\n",
    "\n",
    "entities = nltk.ne_chunk(tagged)\n",
    "entities\n",
    "# Output: Tree('S', [('At', 'IN'), ('eight', 'CD'), (\"o'clock\", 'JJ'), ('on', 'IN'), ('Thursday', 'NNP'), ('morning', 'NN'), ('...', ':'), Tree('PERSON', [('Arthur', 'NNP')]), ('did', 'VBD'), (\"n't\", 'RB'), ('feel', 'VB'), ('very', 'RB'), ('good', 'JJ'), ('.', '.')])\n",
    "\n",
    "t = Tree.fromstring('(S (NP (DT the) (NN cat)) (VP (VBD sat) (PP (IN on) (NP (DT a) (NN mat)))))')\n",
    "t.draw()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the Gutenberg corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gutenberg\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import Tree\n",
    "from nltk.draw import tree\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load and tokenize text data from the Gutenberg corpus\n",
    "emma = gutenberg.raw('austen-emma.txt')\n",
    "tokens = word_tokenize(emma)\n",
    "\n",
    "# Perform basic text preprocessing\n",
    "tokens = [token.lower() for token in tokens if token.isalpha()]  # Convert to lowercase and remove punctuation\n",
    "tokens = [token for token in tokens if token not in stopwords.words('english')]  # Remove stopwords\n",
    "\n",
    "# Perform frequency distribution analysis\n",
    "fdist = FreqDist(tokens)\n",
    "most_common = fdist.most_common(10)\n",
    "print(\"Most common words:\")\n",
    "for word, frequency in most_common:\n",
    "    print(f\"{word}: {frequency}\")\n",
    "\n",
    "# Perform part-of-speech tagging\n",
    "tagged_tokens = nltk.pos_tag(tokens[:100])\n",
    "print(\"\\nPart-of-speech tagging:\")\n",
    "for token, tag in tagged_tokens:\n",
    "    print(f\"{token}: {tag}\")\n",
    "\n",
    "# Perform lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens[:100]]\n",
    "print(\"\\nLemmatization:\")\n",
    "for token, lemma in zip(tokens[:100], lemmatized_tokens):\n",
    "    print(f\"{token}: {lemma}\")\n",
    "\n",
    "# Perform treebank parsing and visualization\n",
    "parsed_sent = Tree.fromstring('(S ' + ' '.join([f'({tag} {token})' for token, tag in tagged_tokens]) + ')')\n",
    "print(\"\\nTreebank Visualization:\")\n",
    "parsed_sent.pretty_print()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data747_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
